server.port=8983

#日志配置文件
logging.config=file:config/log4j2-spring.xml

# AOP
spring.aop.auto=true
spring.aop.proxy-target-class=true
# 批量抓取
spring.kafka.producer.batch-size=65536
# 缓存容量
spring.kafka.producer.buffer-memory=524288
# key/value的序列化
spring.kafka.producer.key-serializer=org.apache.kafka.common.serialization.StringSerializer
spring.kafka.producer.value-serializer=org.apache.kafka.common.serialization.StringSerializer
# 防止消息丢失做以下配置
# 所有follower都响应了才认为消息提交成功
spring.kafka.producer.acks=all
# 这个可以设计为MAX 无限重试
spring.kafka.producer.retries=0
# earliest:当各分区下有已提交的offset时，从提交的offset开始消费；无提交的offset时，从头开始消费
# latest:当各分区下有已提交的offset时，从提交的offset开始消费；无提交的offset时，消费新产生的该分区下的数据
# none:topic各分区都存在已提交的offset时，从offset后开始消费；只要有一个分区不存在已提交的offset，则抛出异常
spring.kafka.consumer.auto-offset-reset=earliest
# false:手动提交ACK true:自动提交
spring.kafka.consumer.enable-auto-commit=true
# 批量消费数量为100，每次取100条消息
# spring.kafka.consumer.max-poll-records=100
# key/value的反序列化
spring.kafka.consumer.key-deserializer=org.apache.kafka.common.serialization.StringDeserializer
spring.kafka.consumer.value-deserializer=org.apache.kafka.common.serialization.StringDeserializer

#FVP揽收路由订阅（43，50，950）kafka地址
monitor_url=http://mom-mon-bus.intsit.sfdc.com.cn:1080/mom-mon/monitor/requestService.pub
#FVP揽收路由订阅（43，50，950） kafka集群名称
cluster_name=bus
#FVP揽收路由订阅（43，50，950） token
fvp_route_token=0000000
#FVP揽收路由订阅（43，50，950）kafka标题
fvp_push_topic=FVP_PUSH_IBU_LOMP_CORE_ER_OUTSIDE

fvp_route_hessian_url=http://localhost:8987/saveFvpRouteMsg